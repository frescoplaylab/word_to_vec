{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to you first hands-on on word embeddings.\n",
    "#### In this hands-on you will be using pretrained GLoVe word vectors from stanford nlp which you can find [here](https://nlp.stanford.edu/projects/glove/)\n",
    "#### Each word vectors is of dimension 50\n",
    "#### You will be performing following operations:\n",
    "    - Load the pretrained vectors from the text file\n",
    "    - Write a function to find cosine similarity between two word vectors\n",
    "    - Write an function to find analogy analogy problems such as King : Queen :: Men : __?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1\n",
    "- A text file having the trained word vectors is provided for you as word_vectors.txt in the same working directory.\n",
    "- Each line in the file is space seperated values where first value is the word and the remaing values are its vector representation.\n",
    "\n",
    "### Define a function get_word_vectors()\n",
    "    parameters: file_name  \n",
    "    returns: word_to_vec: dictionary with key as the word and the value is the corresponding word vectors as 1-d array each element of type float32.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_word_vectors(file_name):\n",
    "    ###Start code here\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###End code\n",
    "    return word_to_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the function you defined above read the word vectors from the file word_vectors.txt and assign it to variable word_to_vec\n",
    "\n",
    "### Expected output  (showing only first few values of vectors)\n",
    "   Father:  [ 0.095496   0.70418   -0.40777   -0.80844    1.256      0.77071 ...]  \n",
    "   mother:  [ 0.4336     1.0727    -0.6196    -0.80679    1.2519     1.3767 ....]  \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec = get_gloVecs('embedding/glove.6B.50d.txt')\n",
    "father = word_to_vec[\"father\"]\n",
    "mother = word_to_vec[\"mother\"]\n",
    "print(\"Father: \", father)\n",
    "print(\"mother: \", mother)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 2 Determine the cosine similarity between two word vectors\n",
    "- The formula for cosine similarity is given by\n",
    "  score = $\\large \\frac{U.V}{\\sqrt{||U||.||V||}}$ where ||U|| and ||V|| is the sum of the squares of the elemnts individual vectors\n",
    "  \n",
    "\n",
    "### Define a function named cosine_similarity()\n",
    "    - parameters u, v are the word vectors whose similarity has to be determined\n",
    "    - returns - score: cosine similarity of u and v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    ###Start code here\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###End code\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the bellow cell to find the similarity between word vectors paris and rome\n",
    "### Expected output\n",
    "   similarity score : 0.7099411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8909039"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paris = word_to_vec[\"paris\"]\n",
    "rome = word_to_vec[\"rome\"]\n",
    "print(\"similarity score :\", cosine_similarity(paris, rome))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "In the word analogy task, we complete the analogy . In detail, we are trying to find a word d, such that the associated word vectors $u_1, v_1, u_2, v_2$ are related in the following manner: $u_1 - v_1 \\approx u_2 - v_2$. We will measure the similarity between $u_1 - v_1$ and $u_2 - v_2$ using cosine similarity.\n",
    "#### As an example,  to find the best possible word for the analogy King : Queen :: Men : __?_ you will perform following steps:\n",
    "- extract word vectors of three words king, queen and men\n",
    "- find the element wise difference between the two word vectors king and queen as V1\n",
    "- Find the element wise difference between the word vector men and each word vector in word_to_vec ditionary as V2 (while doing so exclude the words of interest ie. king, queen and men)\n",
    "- Find the cosine similarity between vector V1 and V2 and choose the word from the word_to_vec ditionary that has maximum similarity between V1 and V2.\n",
    "### Define the function named find_analogy()\n",
    "    - parameters: word1 - string corresponding to word vector $u_1$\n",
    "                  word2 - string corresponding to word vector $v_1$\n",
    "                  word3 - string corresponding to word vector $u_2$\n",
    "                  word_to_vec - dictionary of words and their corresponding vectors\n",
    "    - returns: best_word -  the word such that $u_1$ - $v_1$ is close to $v\\_best\\_word$ - $v_c$, as measured by cosine similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogy(word_1, word_2, word_3, word_to_vec):\n",
    "    ####Start code here\n",
    "    \n",
    "    \n",
    "    ###End code\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below  code to evaluate your above defined function\n",
    "\n",
    "#### Expected output:\n",
    "    father -> son :: mother -> daughter\n",
    "    india -> delhi :: japan -> tokyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('{} -> {} :: {} -> {}'.format('father', 'son', 'mother',find_analogy('father', 'son', 'mother', word_to_vec2)))\n",
    "print ('{} -> {} :: {} -> {}'.format('india', 'delhi', 'japan',find_analogy('india', 'delhi', 'japan', word_to_vec2)))\n",
    "\n",
    "word1 = find_analogy(\"spain\", 'india', 'tokyo', word_to_vec2)\n",
    "word2 = find_analogy(\"small\", 'smaller', 'large', word_to_vec2)\n",
    "\n",
    "with open(\"output.txt\", 'w+') as file:\n",
    "    file.write(word1+'\\n')\n",
    "    file.write(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
