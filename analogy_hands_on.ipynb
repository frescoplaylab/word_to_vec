{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to you first hands-on on word embeddings.\n",
    "#### In this hands-on you will be using pretrained word vectors from stanford nlp which you can find [here](https://nlp.stanford.edu/projects/glove/)\n",
    "#### You will be performing following operations:\n",
    "    - Load the pretrained vectors from the text file\n",
    "    - Write a function to find cosine sililarity between two word vectors\n",
    "    - Write an function to find analogy analogy problems such as King : Queen :: Men : __?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1\n",
    "- A text file having the trained word vectors is provided for you as word_vectors.txt in the same working directory.\n",
    "- Each line in the file has comma seperated values where first value is the word and the remaing values are its vector representation.\n",
    "\n",
    "### Define a function get_word_vectors()\n",
    "    parameters: file_name  \n",
    "    returns: dictionary with key as the word and the value is the corresponding word vectors as 1-d array each element of type float32.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_gloVecs(file_name):\n",
    "    \n",
    "    \n",
    "    \n",
    "    return word_to_glov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the function you defined above read the word vectors from the file word_vectors.txt and assign it to variable word_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec = get_gloVecs('embedding/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Father:  [ 0.095496   0.70418   -0.40777   -0.80844    1.256      0.77071\n",
      " -1.0695     0.76847   -0.87813   -0.0080954  0.43884    1.0476\n",
      " -0.45071   -0.58931    0.83246   -0.038442  -0.73533    0.26389\n",
      "  0.12617    0.57623   -0.23866    1.0922    -0.3367     0.081537\n",
      "  0.84798   -2.4795    -0.40351   -0.84087    0.12034    0.29074\n",
      "  1.9711    -0.50886   -0.45977   -0.13617    0.55613    0.22924\n",
      " -0.18947    0.43544    0.65151    0.043537  -0.1162     0.72196\n",
      " -0.66163   -0.17272    0.27367   -0.28169   -0.82025   -1.5089\n",
      "  0.052787  -0.035579 ]\n",
      "mother:  [ 0.4336     1.0727    -0.6196    -0.80679    1.2519     1.3767\n",
      " -0.93533    0.76088   -0.0056654 -0.063649   0.30297    0.52401\n",
      "  0.2843    -0.38162    0.98797    0.093184  -1.1464     0.070523\n",
      "  0.58012    0.50644   -0.24026    1.7344     0.020735   0.43704\n",
      "  1.2148    -2.2483    -0.41168   -0.24922    0.31225   -0.49464\n",
      "  2.0441    -0.012111  -0.19556    0.085665   0.27682    0.015702\n",
      "  0.0067683  0.12759    0.87008   -0.40641   -0.21057    0.41651\n",
      " -0.021812  -0.53649    0.54095   -0.43442   -0.52489   -2.0277\n",
      "  0.13136    0.11704  ]\n"
     ]
    }
   ],
   "source": [
    "father = word_to_vec[\"father\"]\n",
    "mother = word_to_vec[\"mother\"]\n",
    "print(\"Father: \", father)\n",
    "print(\"mother: \", mother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8909039"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(father, mother)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the word analogy task, we complete the sentence . An example is . In detail, we are trying to find a word d, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner: $e_b - e_a \\approx e_d - e_c$. We will measure the similarity between $e_b - e_a$ and $e_d - e_c$ using cosine similarity.\n",
    "### In this task you will \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogy(word_1, word_2, word_3, word_to_vec):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daughter'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_analogy('father', 'son', 'mother', word_to_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
